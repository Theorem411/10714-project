{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugqA0Huhx6NY"
   },
   "source": [
    "# Integrating needle with TVM\n",
    "\n",
    "This is a demo for our Deep Learning System (10714) course's final project. Our project extends our needle framework with the ability to generate TVMScript, which open the door to many powerful machine learning model optimizations.\n",
    "\n",
    "## Introduction to TVM\n",
    "\n",
    "TVM is an open-source machine learning compiler stack designed to optimize and deploy deep learning models across various hardware platforms. It provides a unified framework for transforming high-level model descriptions into optimized tensor programs that can run efficiently on CPUs, GPUs, and specialized accelerators like TPUs. By integrating TVM into Needle, we unlock several powerful optimization techniques that dramatically improve model performance, including Graph-Level Optimizations, Tensor Program-Level Optimizations, and Cross-Hardware Compatibility. In the following sections, we will delve deeper into each step of the integration process.\n",
    "\n",
    "We will conduct performance experiment on the following models to demonstrate the power of TVM optimizations:\n",
    "\n",
    "-   MLP: A simple feed-forward model that serves as a baseline for optimization comparison.\n",
    "-   Resnet9: A convolutional neural network model often used for image classification tasks.\n",
    "-   Transformer: A model designed for sequence-to-sequence tasks, such as machine translation or text summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c8llRM62MJC"
   },
   "source": [
    "## Approach Overview:\n",
    "\n",
    "### Graph Transpiler: from `ndl.Tensor` to `relax.IRModule`\n",
    "\n",
    "Our translation logic can be founded in `./dlsys/python/needle_tvm/to_tvm.py`.\n",
    "\n",
    "Our main task is to build a graph transpiler that converts needle's computation graph to TVM's `IRModule`. We took advantage of `ndl.Tensor`'s inherent graphical structure to design our translation algorithm. Once a needle model's forward pass is run, we will be able to traverse the tensor graph through a simple topological sort, starting from the output Tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ3eV6lp7EDj"
   },
   "outputs": [],
   "source": [
    "def topological_sort(output):\n",
    "  visited = set()\n",
    "  topo_order = []\n",
    "\n",
    "  def dfs(node):\n",
    "      if node in visited:\n",
    "          return\n",
    "      visited.add(node)\n",
    "      for inp in node.inputs:\n",
    "          dfs(inp)\n",
    "      topo_order.append(node)\n",
    "\n",
    "  dfs(output)\n",
    "  return topo_order  # Reverse for topological order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we traverse the tensor graph, we incrementally build the final `tvm.IRModule` through `tvm.relax.block_builder` API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tvm_tensor(mod: ndl.nn.Module, *args, **kwargs) -> tvm.relax.IRModule:\n",
    "  # set user input Tensor placeholder=True\n",
    "  for t in args: \n",
    "    if isinstance(t, Tensor):\n",
    "      t.placeholder = True\n",
    "      \n",
    "  # topologically sort ndl.Tensor computation graph\n",
    "  topo_order = topological_sort(output_tensor)\n",
    "\n",
    "  # initialize block builder, module inputs/outputs, value to relax.Var map\n",
    "  bb = block_builder.BlockBuilder()\n",
    "  fn_inputs = []\n",
    "  fn_output = None\n",
    "  value_to_var : Dict[ndl.Tensor, relax.Var] = {}\n",
    "\n",
    "\n",
    "  # Create the \"main\" function in emitted IRModule \n",
    "  with bb.function(\"main\"):\n",
    "      with bb.dataflow():\n",
    "          for i, node in enumerate(topo_order):\n",
    "              # Leaf nodes (inputs or constants)\n",
    "              if node.is_leaf():\n",
    "                  if node.placeholder:\n",
    "                      tvm_var = (relax.Var(\"X\", relax.TensorStructInfo(node.shape, \"float32\")))\n",
    "                      value_to_var.setdefault(node, tvm_var)\n",
    "                      fn_inputs.append(tvm_var)\n",
    "                      continue\n",
    "                  else:\n",
    "                      tvm_var = (relax.const(node.numpy(), relax.TensorStructInfo(node.shape, \"float32\")))\n",
    "                      value_to_var.setdefault(node, tvm_var)\n",
    "                      continue\n",
    "\n",
    "              # Map the operation to TVM\n",
    "              tvm_var = node.op.emit_te(bb, value_to_var, node)\n",
    "              value_to_var[node] = tvm_var\n",
    "      \n",
    "          fn_output = bb.emit_output(value_to_var[topo_order[-1]])\n",
    "\n",
    "      bb.emit_func_output(value_to_var[topo_order[-1]], fn_inputs)\n",
    "  return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uses a dictionary that maps `ndl.Tensor` to `relax.Var` to query intermediate translation results. A `relax.Var` represents the result of a computation in `IRModule`. It could be one of three things:\n",
    "\n",
    "1. A placeholder, i.e. model inputs\n",
    "2. A constant, e.g. model parameters\n",
    "3. Result of a tensor operator\n",
    "\n",
    "Not so coincidentally, needle `Tensor` can also be classified into these three categories. The input `Tensor` to user's model will be \"placeholder\" to the `main` TIR function; model weights and biases will be constant (`relax.const`); and since any non-leaf `Tensor` are coupled with a `TensorOp` (the `Tensor.op`), we can translate the operator as a Tensor IR function, and insert a call instruction to said function in the `main` TIR function of the final `IRModule`.\n",
    "\n",
    "For the first kind, we add an boolean attribute `placeholder` to `Value` class (parent of `Tensor`) to indicate if a `Tensor` is an input to user's model. Once detected, our translation algorithm will correspondingly generate a `relax.Var`. This also explains why our `to_tvm_tensor` function does this in the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set user input Tensor placeholder=True\n",
    "for t in args: \n",
    "  if isinstance(t, Tensor):\n",
    "    t.placeholder = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last kind of `Tensor`, we generate the corresponding `TensorIR` function in the `IRModule` using `tvm.topi` operators. In `./dlsys/python/needle/ops/ops_mathematics.py`, we extend every `TensorOp` with a `emit_te` function, e.g. in `EwiseAdd.emit_te`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWiseAdd(TensorOp):\n",
    "    def emit_te(self, bb: relax.BlockBuilder, node_map: Dict[Tensor, relax.Var], node: Tensor) -> relax.Var:\n",
    "        A = node_map[node.inputs[0]]\n",
    "        B = node_map[node.inputs[1]]\n",
    "        \n",
    "        def te_ewise_add(A, B):\n",
    "            return topi.add(A, B)\n",
    "\n",
    "        return bb.emit_te(te_ewise_add, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`relax.BlockBuilder.emit_te` will generate the following TIR function in the final `IRModule`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.prim_func(private=True)\n",
    "def te_ewise_add(lv: T.Buffer((T.int64(32), T.int64(512)), \"float32\"), lv2: T.Buffer((T.int64(32), T.int64(512)), \"float32\"), T_add: T.Buffer((T.int64(32), T.int64(512)), \"float32\")):\n",
    "    T.func_attr({\"tir.noalias\": T.bool(True)})\n",
    "    # with T.block(\"root\"):\n",
    "    for ax0, ax1 in T.grid(T.int64(32), T.int64(512)):\n",
    "        with T.block(\"T_add\"):\n",
    "            v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
    "            T.reads(lv[v_ax0, v_ax1], lv2[v_ax0, v_ax1])\n",
    "            T.writes(T_add[v_ax0, v_ax1])\n",
    "            T_add[v_ax0, v_ax1] = lv[v_ax0, v_ax1] + lv2[v_ax0, v_ax1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Save `IRModule` as Executable\n",
    "\n",
    "Our model compile and evaluation logic can be found in `./dlsys/apps/models/model_eval.py` in class `ModelEval`.\n",
    "\n",
    "The following code builds and runs the `IRModule` transpiled from our `ndl.nn.Module` using TVM `Relax` frontend:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_**:\n",
    "For `nn.Module` that uses have different behavior during inference and training (e.g. `BatchNorm1d`), it's absolutely necessary to run `model.eval()` before calling `to_tvm_tensor` to ensure the transpiled tvm module is indeed from model's inference path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure model is inference mode\n",
    "model.eval()\n",
    "\n",
    "ir_module = to_tvm_tensor(model, ndl.Tensor(x, device=self.ndl_device))\n",
    "module_ex = relax.build(ir_module, target=\"llvm\")\n",
    "module_vm = relax.VirtualMachine(module_ex, self.tvm_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the following code saves the executable as a shared library (`.so`) to be reloaded. For our project we save model executables in `./dlsys/apps/models/module_lib/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_ex.export_library(module_save_path)\n",
    "...\n",
    "module_ex = tvm.runtime.load_module(module_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the compiled TVM module. We check correctness by running needle model side-by-side and compare final activation layer values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ndl = ndl.Tensor(X, device=self.ndl_device, requires_grad=False, placeholder=True)\n",
    "input_tvm = tvm.nd.array(X)\n",
    "\n",
    "ndl_out = self.model(input_ndl)\n",
    "tvm_out = self.module_vm[\"main\"](input_tvm)\n",
    "\n",
    "try: \n",
    "  assert np.allclose(tvm_out.asnumpy(),ndl_out.numpy(), atol=1e-4) # tweak tolerance if fails\n",
    "except AssertionError: \n",
    "  # Compute the absolute difference between two outputs\n",
    "  abs_diff = np.abs(np.linalg.norm(tvm_out.asnumpy()) - np.linalg.norm(ndl_out.numpy()))\n",
    "  print(f\"TVM-NDL diff norm: {abs_diff}\")\n",
    "  raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Program and\n",
    "\n",
    "TVM provides ways to optimize `IRModule` at two level:\n",
    "\n",
    "-   Tensor program level: loop parallelization, tiling, vectorization, etc.\n",
    "-   Computational graph level: operator fusion, layout transformation, memory management.\n",
    "\n",
    "We perform operator fusion on the transpiled `IRModule`. Below is our optimization pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ir_module derived from to_tvm_tensor\n",
    "ir_module = tvm.ir.transform.Sequential([\n",
    "  tvm.relax.transform.LegalizeOps(),\n",
    "  tvm.relax.transform.AnnotateTIROpPattern(),\n",
    "  tvm.relax.transform.FuseOps(),\n",
    "  tvm.relax.transform.FuseTIR(),\n",
    "])(ir_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for tensor program optimizations, we utilize TVM's `meta_schedule` feature to automatically discover optimizations within each TIR function. It enables workload tuning through either custom-defined search spaces or the system's built-in, automatically generated search spaces. In this project, we utilize the autotuning capabilities of meta_schedule to explore and maximize potential performance gains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect number of cores for loop parallelization\n",
    "target = \"llvm\" + f\" -num-cores={os.cpu_count()}\"\n",
    "\n",
    "# Iterate over all functions in the IRModule\n",
    "funcs = 0\n",
    "for func_name in ir_module.get_global_vars():\n",
    "    funcs += 1\n",
    "    if max_funcs is not None and funcs > max_funcs: break\n",
    "    try:\n",
    "        func_name_str = func_name.name_hint\n",
    "        print(f\"tuning: {func_name_str}\")\n",
    "        # Create a tuning database for each function\n",
    "        mod_func = tvm.IRModule.from_expr(ir_module[func_name].with_attr(\"global_symbol\", func_name_str))\n",
    "\n",
    "        # Tune the TIR function\n",
    "        database = meta_schedule.tune_tir(\n",
    "            mod=mod_func,                 # Input module\n",
    "            target=target,                # Target platform (e.g., \"llvm\", \"cuda\")\n",
    "            max_trials_global=5,          # Total tuning trials\n",
    "            num_trials_per_iter=5,        # Trials per tuning iteration\n",
    "            work_dir=f\"{work_dir}/{func_name_str}\",  # Separate logs for each function\n",
    "        )\n",
    "\n",
    "        # Compile the tuned TIR function into a new IRModule\n",
    "        sch = meta_schedule.tir_integration.compile_tir(\n",
    "            database=database,           # The tuning database\n",
    "            mod=mod_func,                # Input module to compile\n",
    "            target=target                # Target platform\n",
    "        )\n",
    "\n",
    "        # Update the module with the tuned function\n",
    "        updated_mod = sch.mod[\"main\"].with_attr(\"global_symbol\", func_name_str)\n",
    "        gv = ir_module.get_global_var(func_name_str)\n",
    "        ir_module.update_func(gv, updated_mod)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although compute-intensive, `meta_schedule` excels at uncovering a wide range of optimizations beyond just tiling. What impressed us most was its remarkable generalizability in identifying optimizations for loop nests of varying shapes and sizes. Unsurprisingly, the optimized IRModule significantly outperforms our needle model, which uses register-tiling exclusively for matrix multiplication kernels.\n",
    "\n",
    "Below is an example of the effects of `meta_schedule` on `reshape`'s TIR function. We were able to see `meta_schedule` discovers loop parallelization, vectorization, and tiling within the loop nest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.prim_func\n",
    "def te_reshape(A: T.Buffer((T.int64(1), T.int64(2048)), \"float32\"), T_reshape: T.Buffer((T.int64(3), T.int64(2048)), \"float32\")):\n",
    "T.func_attr({\"op_pattern\": 2, \"tir.noalias\": T.bool(True)})\n",
    "# with T.block(\"root\"):\n",
    "for ax0_ax1_fused_0 in T.parallel(T.int64(32)):\n",
    "  for ax0_ax1_fused_1 in T.vectorized(T. int64(64)):\n",
    "    with T.block(\"T_reshape\"):\n",
    "      v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "      v_ax1 = T.axis.spatial(T.int64(2048), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_tused_1)\n",
    "      T.reads(A[T.int64(0), v_ax1 % T.Int64(2948)])\n",
    "      T.writes(T_reshape[v_ax0, v_ax1])\n",
    "      T_reshape[vax0, v_ax1] = A[T.int64(0), v_ax1 % T. Int64(2048)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRmXu94w2IT_"
   },
   "source": [
    "# Code Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18747,
     "status": "ok",
     "timestamp": 1733704287462,
     "user": {
      "displayName": "Ziqi Liu",
      "userId": "11174602350199061276"
     },
     "user_tz": 300
    },
    "id": "e3H-DFsd0s60",
    "outputId": "07d38cac-e291-4458-9ce5-451887abea10"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "!git clone https://github.com/Theorem411/10714-project\n",
    "%cd /content/drive/MyDrive/10714/10714-project/\n",
    "\n",
    "!pip3 install pybind11\n",
    "# Install tvm\n",
    "!python3 -m  pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
    "!python -c \"from tvm import relax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ECfOERM3QX2"
   },
   "source": [
    "**_Note_**:\n",
    "while installing `mlc_ai_nightly`, `pip` might prompt user to upgrade certain dependencies. However, we did not encounter any obstacle when we ignored the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH ./dlsys/python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Performance\n",
    "\n",
    "**_Note:_** meta scheduling might take rounghly **_10-15 minutes_** to finish on the first run. However, since we reload the compiled module executable, the second time would be significantly faster as we bypass the meta scheduler.\n",
    "\n",
    "If you want to recompile the model, add `-r` flag when running `tvm_eval.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/10714/project\n",
    "!python dlsys/apps/tvm_eval.py -m='mlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/10714/project\n",
    "!python dlsys/apps/tvm_eval.py -m='transformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet9 Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/10714/project\n",
    "!python dlsys/apps/tvm_eval.py -m='conv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "The results showcase the performance improvements on Intel(R) Xeon(R) CPU (Google Colab environment) achieved through various levels of TVM optimizations on three models: MLP, Transformer, and Conv ResNet9. The key takeaways from the analysis are as follows:\n",
    "\n",
    "| Model        | Needle (Baseline) | TVM (no opt) | TVM (Fusion) | TVM (Fusion + Autotune) |\n",
    "| ------------ | ----------------- | ------------ | ------------ | ----------------------- |\n",
    "| MLP          | 1                 | 1.044256574  | 0.9039618473 | 0.08090549938           |\n",
    "| Transformer  | 1                 | 1.139139027  | 1.124182424  | 0.1171342219            |\n",
    "| Conv ResNet9 | 1                 | 0.3264763352 | 0.3361647851 | 0.1040680634            |\n",
    "\n",
    "<img src=\"./images/cpu_benchmark.png\" alt=\"CPU Benchmark Results\" width=\"70%\">\n",
    "\n",
    "## Explanation\n",
    "\n",
    "**TVM without any optimizations** demonstrates no significant execution time gains for MLP and Transformer models, with MLP showing a slight performance drop (1.04x baseline) and Transformer experiencing a minor slowdown (1.14x baseline). However, Conv ResNet9 benefits considerably, achieving a substantial improvement (0.33x baseline). This discrepancy suggests that while TVM's default handling of tensor operations in the relax framework may implicitly optimize convolution-heavy models like Conv ResNet9, it does not provide similar benefits for fully connected or attention-based models, which rely on different operation patterns.\n",
    "\n",
    "**Operator fusion in TVM** results in modest performance improvements, with mixed outcomes across models. For MLP, there is a slight improvement compared to TVM without optimizations and the baseline, achieving 0.90x baseline performance, but the gain remains limited and far from optimal. For the Transformer, the result is marginally better than TVM with no optimizations (1.12x baseline) but still underperforms compared to the baseline. Conv ResNet9 shows performance similar to TVM without optimization. These results suggest that while operator fusion has potential, its current implementation has limitations, and we aim to revisit and refine the operation fusion design as part of future work.\n",
    "\n",
    "**Combining fusion with autotuning in TVM** results in substantial performance improvements for all models. MLP achieves a remarkable speedup, running at just 0.08x Needle runtime, translating to over 12x faster performance, demonstrating the effectiveness of fine-grained operator tuning. The Transformer sees significant gains, with a runtime of 0.12x Needle, approximately 8.5x faster, highlighting the ability of autotuning to optimize complex operations like matrix multiplications and attention mechanisms. Conv ResNet9 benefits drastically, achieving 0.10x Needle runtime, a 10x improvement, showcasing the impact of autotuning in optimizing convolution-heavy workloads. This combination unlocks the full potential of TVM for diverse workloads.\n",
    "\n",
    "Here are the highlights of the quantitative result:\n",
    "\n",
    "1. MLP benefits the most from TVM's optimizations, particularly autotuning, with a speedup of over 12x compared to the baseline\n",
    "2. Transformer, while more complex, sees notable improvements, especially with autotuning, achieving a speedup of 8.5x\n",
    "3. Conv ResNet9 demonstrates the importance of autotuning for convolution-heavy models, achieving a 10x speedup over the baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlcusion\n",
    "\n",
    "This project demonstrates the seamless integration of the Needle framework with TVM, unlocking advanced optimization capabilities to significantly enhance model performance. By translating Needle's computational graph into TVM's IRModule, we leverage graph-level and tensor program-level optimizations to achieve efficient execution across diverse hardware platforms.\n",
    "\n",
    "Our evaluation demonstrates the performance impact of integrating TVM with the Needle framework across three models: MLP, Transformer, and a convolutional ResNet9. Initially, without any optimizations, TVM underperforms compared to the baseline. However, with operator fusion enabled, the performance aligns closely with the baseline. The true potential of TVM is unlocked through autotuning, which delivers an impressive 8–12x speedup compared to the baseline. These results highlight the transformative power of TVM’s advanced optimization capabilities, emphasizing the value of its integration into lightweight deep learning frameworks like Needle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SNu-EJ_7k4m"
   },
   "source": [
    "# Reference:\n",
    "\n",
    "[1] Apache TVM. TVM Documentation. Available at: [https://tvm.apache.org/docs/](https://tvm.apache.org/docs/)  \n",
    "[2] Machine Learning Compilation. Online course developed by Tianqi Chen. Available at: [https://mlc.ai](https://mlc.ai)  \n",
    "[3] PyTorch torch.fx. PyTorch Torch.fx Documentation. Available at: [https://pytorch.org/docs/stable/fx.html](https://pytorch.org/docs/stable/fx.html)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
