{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. function to map ndl tensors to tvm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relax\n",
    "\n",
    "def convert_tensor_to_tvm(tensor):\n",
    "    numpy_array = tensor.numpy() \n",
    "    return tvm.nd.array(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Map ndl ops to tvm ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax import op\n",
    "def map_op_to_tvm(op):\n",
    "    op_mapping = {\n",
    "        \"add\": op.add,\n",
    "        \"matmul\": op.matmul,\n",
    "        \"relu\": op.relu,\n",
    "        # we'd add any other ops here any ops from ops.mathematic or ops.logarithmic\n",
    "    }\n",
    "    if op.__class__.__name__ not in op_mapping:\n",
    "        raise ValueError(f\"Unsupported operation: {op.__class__.__name__}\")\n",
    "    return op_mapping[op.__class__.__name__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. traverse the ndl graph and generate tvm code, since we have mappings from ndl to tvm for both tensors and ops, we can generate the tvm code from ndl graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax import Function, Var, block_builder\n",
    "from collections import deque\n",
    "\n",
    "def convert_graph_to_tvm(output_tensor):\n",
    "    bb = block_builder.BlockBuilder()\n",
    "\n",
    "    # Map of `Value` nodes to Relax variables\n",
    "    value_to_var = {}\n",
    "\n",
    "    # Topological sort of the graph\n",
    "    def topological_sort(output):\n",
    "        visited = set()\n",
    "        topo_order = []\n",
    "\n",
    "        def dfs(node):\n",
    "            if node in visited:\n",
    "                return\n",
    "            visited.add(node)\n",
    "            for inp in node.inputs:\n",
    "                dfs(inp)\n",
    "            topo_order.append(node)\n",
    "\n",
    "        dfs(output)\n",
    "        return reversed(topo_order)  # Reverse for topological order\n",
    "\n",
    "    # Process the graph in topological order\n",
    "    topo_order = topological_sort(output_tensor)\n",
    "    for node in topo_order:\n",
    "        # Leaf nodes (inputs or constants)\n",
    "        if node.is_leaf():\n",
    "            tvm_var = bb.emit_var(relax.Var(node.op.__class__.__name__, shape=node.shape))\n",
    "            value_to_var[node] = tvm_var\n",
    "            continue\n",
    "\n",
    "        # Map the operation to TVM\n",
    "        tvm_op = map_op_to_tvm(node.op)\n",
    "\n",
    "        # Get TVM inputs by recursively converting dependencies\n",
    "        tvm_inputs = [value_to_var[inp] for inp in node.inputs]\n",
    "\n",
    "        # Emit the Relax operation\n",
    "        tvm_var = bb.emit(tvm_op(*tvm_inputs))\n",
    "        value_to_var[node] = tvm_var\n",
    "\n",
    "    # Create the Relax function\n",
    "    with bb.function(\"main\"):\n",
    "        bb.emit_output(value_to_var[output_tensor])\n",
    "    return bb.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
